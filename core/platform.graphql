"""
Direct the client to resolve this field locally, either from the cache or local resolvers.
"""
directive @client(
  """
  When true, the client will never use the cache for this value. See
  https://www.apollographql.com/docs/react/essentials/local-state/#forcing-resolvers-with-clientalways-true
  """
  always: Boolean
) on FIELD | FRAGMENT_DEFINITION | INLINE_FRAGMENT

"""
Export this locally resolved field as a variable to be used in the remainder of this query. See
https://www.apollographql.com/docs/react/essentials/local-state/#using-client-fields-as-variables
"""
directive @export(
  """The variable name to export this field as."""
  as: String!
) on FIELD

"""
Specify a custom store key for this result. See
https://www.apollographql.com/docs/react/advanced/caching/#the-connection-directive
"""
directive @connection(
  """Specify the store key."""
  key: String!

  """
  An array of query argument names to include in the generated custom store key.
  """
  filter: [String!]
) on FIELD

type Query {
  search(input: SearchInput!): SearchResults!
  answer(input: QuestionInput!): Answers!
}

input SearchInput {
  questions: [String!]!

  """Count of passages to return, per question"""
  passages_count: Int = 10
}

type SearchResults {
  nodes: [SearchResult!]!
}

type SearchResult {
  question: String!

  """Passages of texts for question"""
  passages: [Passage!]!
}

type Passage {
  datasets_id: Int

  """Id of entity in wikidata, can be linked to wikipedia page"""
  wiki_id: String

  """Score of relativity passage to answer, bigger better"""
  score: Float
  start_paragraph: Int
  end_paragraph: Int
  start_character: Int
  end_character: Int
  article_title: String!
  section_title: String!
  passage_text: String!
}

input QuestionInput {
  """Question on which need to answer"""
  question: String!

  """Context for given question"""
  context: String!

  """Summary maximum length of question and context document"""
  max_input_length: Int = 1024

  """count of answers to generate per question"""
  num_answers: Int = 10

  """Number of beams for beam search. 1 means no beam search"""
  num_beams: Int = 8

  """minimum length of answer"""
  min_answer_length: Int = 64

  """maximum length of answer"""
  max_answer_length: Int = 256

  """Whether or not to use sampling ; use greedy decoding otherwise"""
  do_sample: Boolean = false

  """The value used to module the next token probabilities"""
  temperature: Float = 1

  """
  if set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.
  """
  top_p: Float = 1

  """
  The number of highest probability vocabulary tokens to keep for top-k-filtering
  """
  top_k: Int = 50

  """if set to int > 0, all ngrams of that size can only occur once."""
  no_repeat_ngram_size: Int = 3

  """
  Exponential penalty to the length. 1.0 means no penalty. Set to values < 1.0 in order to encourage the model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer sequences.
  """
  length_penalty: Float = 1

  """
  The maximum amount of time you allow the computation to run for in seconds. generation will still finish the current pass after allocated time has been passed.
  """
  max_time: Float
}

type Answers {
  """List of answers on questions"""
  nodes: [Answer!]!
}

type Answer {
  question: String!
  text: String!
}
